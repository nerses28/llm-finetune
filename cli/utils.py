from argparse import Namespace


accelerate_args = Namespace(
    config_file="<INSERT>",
    quiet=None,
    cpu=False,
    multi_gpu=False,
    tpu=False,
    ipex=False,
    mixed_precision="no",
    num_processes=1,
    num_machines=1,
    num_cpu_threads_per_process=1,
    enable_cpu_affinity=False,
    dynamo_backend="no",
    dynamo_mode="default",
    dynamo_use_fullgraph=False,
    dynamo_use_dynamic=False,
    use_deepspeed=False,
    use_fsdp=True,
    use_megatron_lm=False,
    use_xpu=False,
    gpu_ids="all",
    same_network=False,
    machine_rank=0,
    main_process_ip=None,
    main_process_port=None,
    tee="0",
    role="default",
    rdzv_backend="static",
    rdzv_conf="",
    max_restarts=0,
    monitor_interval=5,
    module=None,
    no_python=None,
    tpu_use_cluster=False,
    tpu_use_sudo=False,
    vm=None,
    env=None,
    main_training_function="main",
    downcast_bf16=False,
    deepspeed_config_file=None,
    zero_stage=None,
    ffload_optimizer_device=None,
    offload_param_device=None,
    offload_optimizer_nvme_path=None,
    offload_param_nvme_path=None,
    gradient_accumulation_steps=None,
    gradient_clipping=None,
    zero3_init_flag=None,
    zero3_save_16bit_model=None,
    deepspeed_hostfile=None,
    deepspeed_exclusion_filter=None,
    deepspeed_inclusion_filter=None,
    deepspeed_multinode_launcher=None,
    deepspeed_moe_layer_cls_names=None,
    fsdp_offload_params=False,
    fsdp_min_num_params=100000000.0,
    fsdp_sharding_strategy="FULL_SHARD",
    fsdp_auto_wrap_policy="TRANSFORMER_BASED_WRAP",
    fsdp_transformer_layer_cls_to_wrap=None,
    fsdp_backward_prefetch_policy=None,
    fsdp_backward_prefetch="BACKWARD_PRE",
    fsdp_state_dict_type="SHARDED_STATE_DICT",
    fsdp_forward_prefetch=False,
    fsdp_use_orig_params=False,
    fsdp_cpu_ram_efficient_loading=True,
    fsdp_sync_module_states=True,
    megatron_lm_tp_degree=1,
    megatron_lm_pp_degree=1,
    megatron_lm_num_micro_batches=None,
    megatron_lm_sequence_parallelism=None,
    megatron_lm_recompute_activations=None,
    megatron_lm_use_distributed_optimizer=None,
    megatron_lm_gradient_clipping=1.0,
    aws_access_key_id=None,
    aws_secret_access_key=None,
    debug=False,
    training_script="<INSERT>",
    mpirun_hostfile=None,
    mpirun_ccl=1,
    training_script_args=[],
    use_cpu=False,
    tpu_name=None,
    tpu_zone=None,
    command_file=None,
    commands=None,
    tpu_vm=None,
    tpu_env=[],
)


training_args = dict(
    model_name_or_path="meta-llama/Meta-Llama-3.1-8B-Instruct",
    #model_name_or_path="BramVanroy/fietje-2",
    logging_steps=5,
    log_level="info",
    logging_strategy="steps",
    eval_strategy="epoch",
    save_strategy="no",
    bf16="True",
    lr_scheduler_type="cosine",
    weight_decay="1e-4",
    warmup_ratio="0.1",
    max_grad_norm="1.0",
    output_dir="test",
    gradient_accumulation_steps="1",
    gradient_checkpointing="True",
    use_reentrant="True",
    use_flash_attn="False",
    use_peft_lora="True",
    lora_target_modules="all-linear",
)
