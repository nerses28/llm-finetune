from argparse import Namespace


accelerate_args = Namespace(
    config_file='/clips-ai-train/cli/data/fsdp_config_qlora.yaml', 
    quiet=None, 
    cpu=False, 
    multi_gpu=False, 
    tpu=False, 
    ipex=False, 
    mixed_precision='no', 
    num_processes=2, 
    num_machines=1, 
    num_cpu_threads_per_process=1, 
    enable_cpu_affinity=False, 
    dynamo_backend='no', 
    dynamo_mode='default', 
    dynamo_use_fullgraph=False, 
    dynamo_use_dynamic=False, 
    use_deepspeed=False, 
    use_fsdp=True, 
    use_megatron_lm=False, 
    use_xpu=False, 
    gpu_ids='all', 
    same_network=False, 
    machine_rank=0, 
    main_process_ip=None, 
    main_process_port=None, 
    tee='0', 
    role='default', 
    rdzv_backend='static', 
    rdzv_conf='', 
    max_restarts=0, 
    monitor_interval=5, 
    module=None, 
    no_python=None, 
    tpu_use_cluster=False, 
    tpu_use_sudo=False, 
    vm=None, 
    env=None, 
    main_training_function='main', 
    downcast_bf16=False, 
    deepspeed_config_file=None, 
    zero_stage=None,
    ffload_optimizer_device=None, 
    offload_param_device=None, 
    offload_optimizer_nvme_path=None, 
    offload_param_nvme_path=None, 
    gradient_accumulation_steps=None, 
    gradient_clipping=None, 
    zero3_init_flag=None, 
    zero3_save_16bit_model=None, 
    deepspeed_hostfile=None, 
    deepspeed_exclusion_filter=None, 
    deepspeed_inclusion_filter=None, 
    deepspeed_multinode_launcher=None, 
    deepspeed_moe_layer_cls_names=None, 
    fsdp_offload_params=False, 
    fsdp_min_num_params=100000000.0,
    fsdp_sharding_strategy='FULL_SHARD', 
    fsdp_auto_wrap_policy='TRANSFORMER_BASED_WRAP', 
    fsdp_transformer_layer_cls_to_wrap=None, 
    fsdp_backward_prefetch_policy=None, 
    fsdp_backward_prefetch='BACKWARD_PRE', 
    fsdp_state_dict_type='SHARDED_STATE_DICT', 
    fsdp_forward_prefetch=False, 
    fsdp_use_orig_params=False, 
    fsdp_cpu_ram_efficient_loading=True, 
    fsdp_sync_module_states=True, 
    megatron_lm_tp_degree=1, 
    megatron_lm_pp_degree=1, 
    megatron_lm_num_micro_batches=None, 
    megatron_lm_sequence_parallelism=None, 
    megatron_lm_recompute_activations=None, 
    megatron_lm_use_distributed_optimizer=None, 
    megatron_lm_gradient_clipping=1.0, 
    aws_access_key_id=None, 
    aws_secret_access_key=None, 
    debug=False, 
    training_script='/clips-ai-train/cli/scripts/train.py', 
    mpirun_hostfile=None, 
    mpirun_ccl=1, 
    training_script_args=['--seed', '100', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B-Instruct', '--train_filename', '/clips-ai-train/datasets/ultrachat/train.jsonl', '--valid_filename', '/clips-ai-train/datasets/ultrachat/validation.jsonl', '--add_special_tokens', 'False', '--append_concat_token', 'True', '--max_seq_len', '1024', '--num_train_epochs', '1.0', '--logging_steps', '5', '--log_level', 'info', '--logging_strategy', 'steps', '--eval_strategy', 'epoch', '--save_strategy', 'no', '--bf16', 'True', '--packing', 'True', '--learning_rate', '5e-05', '--lr_scheduler_type', 'cosine', '--weight_decay', '1e-4', '--warmup_ratio', '0.1', '--max_grad_norm', '1.0', '--output_dir', 'llama-sft-qlora-fsdp', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing', 'True', '--use_reentrant', 'True', '--use_flash_attn', 'True', '--use_peft_lora', 'True', '--lora_r', '32', '--lora_alpha', '64', '--lora_dropout', '0.0', '--lora_target_modules', 'all-linear', '--use_4bit_quantization', 'True', '--use_nested_quant', 'True', '--bnb_4bit_compute_dtype', 'bfloat16', '--bnb_4bit_quant_storage_dtype', 'bfloat16'],  
    use_cpu=False, 
    tpu_name=None, 
    tpu_zone=None, 
    command_file=None, 
    commands=None, 
    tpu_vm=None, 
    tpu_env=[]
)


training_args = dict(
    model_name_or_path="meta-llama/Meta-Llama-3-8B-Instruct",
    logging_steps=5,
    log_level="info",
    logging_strategy="steps",
    eval_strategy="epoch",
    save_strategy="no",
    bf16="True",
    lr_scheduler_type="cosine",
    weight_decay="1e-4",
    warmup_ratio="0.1",
    max_grad_norm="1.0",
    output_dir="test",
    gradient_accumulation_steps="1",
    gradient_checkpointing="True",
    use_reentrant="True",
    use_flash_attn="True",
    use_peft_lora="True",
    lora_target_modules="all-linear",
    use_4bit_quantization="True",
    use_nested_quant="True",
    bnb_4bit_compute_dtype="bfloat16",
    bnb_4bit_quant_storage_dtype="bfloat16",
)